{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Noa Flaherty\n",
    "noaflaherty@gmail.com\n",
    "10/7/2018\n",
    "\n",
    "## Table of Contents\n",
    "### Phase 1: Data Parsing and Transformation\n",
    "-  __Step 1:__ CSV Parsing and Day-Level Summarization\n",
    "-  __Step 2:__ Generate Time-Series Dataframe\n",
    "-  __Step 3:__ Compute Rolling Metrics\n",
    "-  __Step 4:__ Generate \"Target\" Column\n",
    "-  __Step 5:__ Filter Out Ineligibile Dates & Drop Unneeded Columns\n",
    "-  __Result:__ Final Dataframe Output\n",
    "\n",
    "### Phase 2: Dummy Model Creation\n",
    "\n",
    "### Phase 3: Model Evaluation & Comparison\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 0: Model Evaluator Inputs\n",
    "\n",
    "These are the four key inputs for this model evaluation framework. You must specify your input csv and your two model file names. They must be located in model-evaluation/notebook/stored_csvs/ and model-evaluation/notebook/stored_models/ respectively.\n",
    "\n",
    "Lastly, you must specify whether you want to generate dummy models to compare or compare your own. If CREATE_DUMMY_MODELS = True, then two models will be created with the above-specified file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_FILENAME = 'TransactionsCompany1.csv'\n",
    "MODEL_1_FILENAME = 'model_1.sav'\n",
    "MODEL_2_FILENAME = 'model_2.sav'\n",
    "CREATE_DUMMY_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Additional Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (model_evaluation.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/noaflaherty/Documents/GitHub/model-evaluation/server/utils/model_evaluation.py\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    print \"Model Name: %s\\n\" if model_name else print \"\\n\"\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "# Import relative packages from utils\n",
    "path_to_project = path.dirname( path.dirname( path.abspath('__file__') ) )\n",
    "sys.path.append( path_to_project )\n",
    "from server.utils import model_evaluation, data_processing\n",
    "\n",
    "\n",
    "# File path to input csv\n",
    "input_csv = \"{PROJECT_PATH}/notebook/stored_csvs/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=INPUT_CSV_FILENAME)\n",
    "\n",
    "# After processing the CSV, a pickle file will be saved here of the dataframe for fast retrieval.\n",
    "pickle_file_of_df = \"{PROJECT_PATH}/notebook/stored_dataframes/{FILE_NAME}.pkl\".format(PROJECT_PATH=path_to_project, FILE_NAME=INPUT_CSV_FILENAME.split('.')[0])\n",
    "\n",
    "# File paths for model pickle files\n",
    "model_1_file = \"{PROJECT_PATH}/notebook/stored_models/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=MODEL_1_FILENAME)\n",
    "model_2_file = \"{PROJECT_PATH}/notebook/stored_models/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=MODEL_2_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# Set pandas dataframe print options\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "# pd.set_option('display.width', 500)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Parsing & Transformation\n",
    "The goal of this phase is to read an input csv representing an event stream and output a time-series'ed dataframe for use in dummy model training and model evaluation.\n",
    "\n",
    "The final dataframe outputted at the end of this phase has the following properties:\n",
    "-  One row per customer per day, where:\n",
    "    -  The first date for a given customer is the first day on which they had an event\n",
    "    -  The maximum date for ALL customers is the maximum date across all events across all customers (if this was a live feed of data rather than a static csv, we might consider just making this \"today.\")\n",
    "    -  We include all days between and including the above min and max dates for each customer\n",
    "    -  We then filter out dates that are \"too young\" or \"too old\" to be useful for our model (described further in Step 4).\n",
    "-  An index of ['customer_id', 'date']\n",
    "-  Generated columns that represent computed metrics for use as model factors (e.g. purchase_value_l30d is the rolling 30 day sum of purchase values for a given customer on a given day)\n",
    "-  The rightmost column is our target value: whether or not the customer made one or more purchases in the 6 months following that day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: CSV Parsing and Day-Level Summarization\n",
    "\n",
    "The first step is to read in the specified CSV, which represents an event stream, and perform the first level of data transformation. Our goal is to have one row per customer per day on which one or more events occured, with columns that aggregate the total value of the events for that customer on that day as well as the count of events that customer had on that day.\n",
    "\n",
    "-  Two columns new for the given event type (in this case, purchase events).\n",
    "    -  The first is a sum of the event values for all events of that event type on that day for that customer (e.g. total_purchase_value_on_day)\n",
    "    -  The second is a count of all events of that event type on that day for that customer (e.g. purchase_count_on_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_summarized_event_stream = data_processing.read_event_stream_data_set(input_csv, 'purchase')\n",
    "print df_summarized_event_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Time Series Dataframe\n",
    "This next step is to take the dataframe from Step 1, which contains one row per customer per day on which an event occured, and extend it to be one row per customer per day since their first event. This makes it so that we can compute rolling metrics for each day and make it easy to see what a given customer looked like on a given day. We keep the first_event_timestamp as a column for later data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = data_processing.generate_time_series(df_summarized_event_stream, event_names=['purchase'])\n",
    "print df_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute Rolling Metrics\n",
    "In this step, we compute rolling metrics for use as input factors for models. For now, we simply compute the rolling sum of event values (e.g. total purchase value) and count of events (e.g. number of purchase events) with window sizes of 1, 3, and 6 month intervals (assuming 30 days per month). \n",
    "\n",
    "This could be a good area for future refinement if we wanted to develop more sophisticated models. For example, you could imagine additional columns for use as factors such as: number of previous consecutive months with one or more purchases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series_w_rolling_metrics = data_processing.generate_metrics_for_use_as_factors(df_time_series)\n",
    "print df_time_series_w_rolling_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate \"Target\" Column\n",
    "This creates a column representing what we are trying to predict. If a customer made one or more purchases in the 6 months following that day-row, they get a 1 on that day, otherwise, they get a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_into_future_in_days =  6*30 # Number of days into the future to look. Set here to 6 months, assuming 30 days per month\n",
    "df_time_series_w_target_col = data_processing.determine_if_event_occured_in_furture_x_days(df_time_series_w_rolling_metrics, 'purchase', rolling_window_into_future_in_days)\n",
    "print df_time_series_w_target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Filter Out Ineligibile Dates & Drop Unneeded Columns\n",
    "From Step 3, we know that we look some number of days into the past to compute rolling metrics (in this case, the largest is 180 days) and in Step 4 we say that we look some number of days into the future for our target column (also 180 days in this scenario). Therefore, some days will be \"too young\" to generate meaningful rolling metrics and some dates will be \"too old\" to have a full 180 window in the future that still exists within the timeframe covered in the dataset.\n",
    "\n",
    "To account for these sets of dates that are \"too young\" or \"too old,\" we generate columns and then use them to filter out ineligible rows to create our final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rolling_window_span_in_days = 6*30 # This should be set to the same number of days as our longest rolling metric (in this case, 180 days)\n",
    "\n",
    "df_time_series_w_age_viability = data_processing.determine_age_viability_for_model(df_time_series_w_target_col, max_rolling_window_span_in_days, rolling_window_into_future_in_days)\n",
    "df_final = data_processing.get_eligible_training_set(df_time_series_w_age_viability)\n",
    "\n",
    "# Save to a pickle file for easy retrieval later.\n",
    "if pickle_file_of_df:\n",
    "    df_final.to_pickle(pickle_file_of_df)\n",
    "    print \"Saved dataframe to %s.\" % pickle_file_of_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Final Dataframe Output\n",
    "This is the final output of Phase 1. It is a clean time-series'ed dataframe that can be used for training or evaluating models.\n",
    "\n",
    "You can either run all cells above, or just this one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = data_processing.load_dataframe(pickle_file_of_df, csv=input_csv, overwrite_pickle=False)\n",
    "print df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Phase 2: Dummy Model Creation\n",
    "\n",
    "For now, we will simply create two very similar Logistic Regression models, who differ only in their seed and training/test split values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [model_1_file, model_2_file]\n",
    "models = model_evaluation.load_models(df_final, model_paths, generate_new=CREATE_DUMMY_MODELS) # generate_new can be set to False to simply load models from the inputs at the top.\n",
    "print models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Model Evaluation & Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_results = model_evaluation.compare_models(input_csv, model_paths, df_pickle_file=pickle_file_of_df, generate_new_models=False)\n",
    "\n",
    "# Print results in easily-readable form\n",
    "for idx, model_name in enumerate(model_comparison_results):\n",
    "        model_results = model_comparison_results[model_name]\n",
    "        print_model_evaluation_metrics(model_results, idx+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "### Data Parsing & Transformation:\n",
    "Ideally this event stream data would already live in our database rather than be batch-uploaded through a CSV. If this were the case, then we could run a job at the end of each day to incrementally produce our needed time-series data as each day goes by. This would have the following benefits:\n",
    " -  The user would not have to wait at all for their event data to process and be transformed\n",
    " -  We could optimize how we transform our data to look through a much smaller window of rows, rather than create it from scratch every time\n",
    " \n",
    "While the code has been genericized to a point, more could be done to further extend this framework to other event streams. Ideally you could:\n",
    " -  Specify one or more csvs and the \"event name\" for each;\n",
    " -  If two represented the same event type, we'd take the union and treat them as if just one csv of that event type was uploaded;\n",
    " -  For each event type, we could dynamically generate additional rolling metric columns for our time-series dataframe. These could be used as additional model inputs (i.e. factors).\n",
    " -  You could specify which event type you want to predict for\n",
    "\n",
    "\n",
    "### Interacting w/ the Model Evaluation Framework:\n",
    "Currently, a user would interact with this model evaluation framework through this Jupyter notebook or through importing these methods into their own code. This could be extended to be its own REST endpoint, or its own whole web-app with a front end.\n",
    "\n",
    "\n",
    "### Scalability: Supporting 100,000 Companies\n",
    "Much of the wins for scalability would come from the \"Data Parsing & Transformation\" section just above. Much computation and waiting goes into the transformation of the event stream. However, there is also significant wait time when computing the evaluation metrics. To address this, let's imagine we've implemented the data transformation recommendations above and wanted to create a REST POST endpoint for model evaluation. You post to it two .sav model files and want it to return the evaluation metrics of each. \n",
    "\n",
    "Once the web server received the request, I would have it offload the work of computing the evaluation metrics to a worker (say, RabbitMQ). Ideally, in the post I also include a callback url. Once the worker is done computing the model evaluation metrics, it could post the results back to that callback url.\n",
    "\n",
    "Even in a world where the event stream does not live in our database and we needed to accept csvs from customers, we could use this pattern of offloading the work to worker queue and post the results back to a callback url.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
