{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Noa Flaherty\n",
    "noaflaherty@gmail.com\n",
    "10/7/2018\n",
    "\n",
    "## Table of Contents\n",
    "### Phase 1: Data Parsing and Transformation\n",
    "-  __Step 1:__ CSV Parsing and Day-Level Summarization\n",
    "-  __Step 2:__ Generate Time-Series Dataframe\n",
    "-  __Step 3:__ Compute Rolling Metrics\n",
    "-  __Step 4:__ Generate \"Target\" Column\n",
    "-  __Step 5:__ Filter Out Ineligibile Dates & Drop Unneeded Columns\n",
    "-  __Result:__ Final Dataframe Output\n",
    "\n",
    "### Phase 2: Dummy Model Creation\n",
    "\n",
    "### Phase 3: Model Evaluation & Comparison\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 0: Model Evaluator Inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_FILENAME = 'TransactionsCompany1.csv' # Make sure that file is placed in model-evaluation/notebook/stored/csvs\n",
    "MODEL_1_FILENAME = 'model_1.sav'\n",
    "MODEL_2_FILENAME = 'model_2.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "# Import relative packages from utils\n",
    "path_to_project = path.dirname( path.dirname( path.abspath('__file__') ) )\n",
    "sys.path.append( path_to_project )\n",
    "from server.utils import model_evaluation, data_processing\n",
    "\n",
    "\n",
    "# File path to input csv\n",
    "input_csv = \"{PROJECT_PATH}/notebook/stored_csvs/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=INPUT_CSV_FILENAME)\n",
    "\n",
    "# After processing the CSV, a pickle file will be saved here of the dataframe for fast retrieval.\n",
    "pickle_file_of_df = \"{PROJECT_PATH}/notebook/stored_dataframes/{FILE_NAME}.pkl\".format(PROJECT_PATH=path_to_project, FILE_NAME=INPUT_CSV_FILENAME.split('.')[0])\n",
    "\n",
    "# File paths for model pickle files\n",
    "model_1_file = \"{PROJECT_PATH}/notebook/stored_models/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=MODEL_1_FILENAME)\n",
    "model_2_file = \"{PROJECT_PATH}/notebook/stored_models/{FILE_NAME}\".format(PROJECT_PATH=path_to_project, FILE_NAME=MODEL_2_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# Set pandas dataframe print options\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "# pd.set_option('display.width', 500)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Parsing & Transformation\n",
    "The goal of this phase is to read an input csv representing an event stream and output a time-series'ed dataframe for use in dummy model training and model evaluation.\n",
    "\n",
    "The final dataframe outputted at the end of this phase has the following properties:\n",
    "-  One row per customer per day, where:\n",
    "    -  The first date for a given customer is the first day on which they had an event\n",
    "    -  The maximum date for ALL customers is the maximum date across all events across all customers (if this was a live feed of data rather than a static csv, we might consider just making this \"today.\")\n",
    "    -  We include all days between and including the above min and max dates for each customer\n",
    "    -  We then filter out dates that are \"too young\" or \"too old\" to be useful for our model (described further in Step 4).\n",
    "-  An index of ['customer_id', 'date']\n",
    "-  Generated columns that represent computed metrics for use as model factors (e.g. purchase_value_l30d is the rolling 30 day sum of purchase values for a given customer on a given day)\n",
    "-  The rightmost column is our target value: whether or not the customer made one or more purchases in the 6 months following that day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: CSV Parsing and Day-Level Summarization\n",
    "\n",
    "The first step is to read in the specified CSV, which represents an event stream, and perform the first level of data transformation. Our goal is to have one row per customer per day on which one or more events occured, with columns that aggregate the total value of the events for that customer on that day as well as the count of events that customer had on that day.\n",
    "\n",
    "-  Two columns new for the given event type (in this case, purchase events).\n",
    "    -  The first is a sum of the event values for all events of that event type on that day for that customer (e.g. total_purchase_value_on_day)\n",
    "    -  The second is a count of all events of that event type on that day for that customer (e.g. purchase_count_on_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      customer_id       date  total_purchase_value_on_day  purchase_count_on_day\n",
      "0      000794e900 2015-06-02                       145.14                      1\n",
      "1      000794e900 2015-06-11                        39.25                      1\n",
      "2      000794e900 2015-07-04                       102.11                      1\n",
      "3      000794e900 2017-05-15                       110.13                      1\n",
      "4      000794e900 2017-05-25                        68.71                      1\n",
      "5      000794e900 2017-05-26                       101.50                      1\n",
      "6      000794e900 2018-02-12                        73.64                      1\n",
      "7      000794e900 2018-02-18                        46.83                      1\n",
      "8      0007e55b29 2017-05-13                        15.20                      1\n",
      "9      0017117287 2018-04-10                        73.99                      1\n",
      "...           ...        ...                          ...                    ...\n",
      "22068  ffea6633ab 2018-05-30                        64.00                      1\n",
      "22069  ffed4fd9aa 2017-09-15                        78.85                      2\n",
      "22070  ffefc14023 2017-02-03                        49.00                      1\n",
      "22071  fff7a1634d 2018-03-30                        74.00                      1\n",
      "22072  fffcaa3024 2014-10-25                        28.50                      1\n",
      "22073  fffcaa3024 2014-11-29                        69.31                      2\n",
      "22074  fffcaa3024 2015-02-09                        46.24                      1\n",
      "22075  fffcaa3024 2015-03-09                        33.88                      1\n",
      "22076  fffcaa3024 2015-04-09                        53.75                      1\n",
      "22077  ffffe95b20 2017-08-09                        32.49                      1\n",
      "\n",
      "[22078 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_summarized_event_stream = data_processing.read_event_stream_data_set(input_csv, 'purchase')\n",
    "print df_summarized_event_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Time Series Dataframe\n",
    "This next step is to take the dataframe from Step 1, which contains one row per customer per day on which an event occured, and extend it to be one row per customer per day since their first event. This makes it so that we can compute rolling metrics for each day and make it easy to see what a given customer looked like on a given day. We keep the first_event_timestamp as a column for later data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       first_event_timestamp  total_purchase_value_on_day  purchase_count_on_day\n",
      "customer_id date                                                                                \n",
      "000794e900  2015-06-02            2015-06-02                       145.14                    1.0\n",
      "            2015-06-03            2015-06-02                         0.00                    0.0\n",
      "            2015-06-04            2015-06-02                         0.00                    0.0\n",
      "            2015-06-05            2015-06-02                         0.00                    0.0\n",
      "            2015-06-06            2015-06-02                         0.00                    0.0\n",
      "            2015-06-07            2015-06-02                         0.00                    0.0\n",
      "            2015-06-08            2015-06-02                         0.00                    0.0\n",
      "            2015-06-09            2015-06-02                         0.00                    0.0\n",
      "            2015-06-10            2015-06-02                         0.00                    0.0\n",
      "            2015-06-11            2015-06-02                        39.25                    1.0\n",
      "...                                      ...                          ...                    ...\n",
      "ffffe95b20  2018-05-24            2017-08-09                         0.00                    0.0\n",
      "            2018-05-25            2017-08-09                         0.00                    0.0\n",
      "            2018-05-26            2017-08-09                         0.00                    0.0\n",
      "            2018-05-27            2017-08-09                         0.00                    0.0\n",
      "            2018-05-28            2017-08-09                         0.00                    0.0\n",
      "            2018-05-29            2017-08-09                         0.00                    0.0\n",
      "            2018-05-30            2017-08-09                         0.00                    0.0\n",
      "            2018-05-31            2017-08-09                         0.00                    0.0\n",
      "            2018-06-01            2017-08-09                         0.00                    0.0\n",
      "            2018-06-02            2017-08-09                         0.00                    0.0\n",
      "\n",
      "[6551751 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_time_series = data_processing.generate_time_series(df_summarized_event_stream, event_names=['purchase'])\n",
    "print df_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute Rolling Metrics\n",
    "In this step, we compute rolling metrics for use as input factors for models. For now, we simply compute the rolling sum of event values (e.g. total purchase value) and count of events (e.g. number of purchase events) with window sizes of 1, 3, and 6 month intervals (assuming 30 days per month). \n",
    "\n",
    "This could be a good area for future refinement if we wanted to develop more sophisticated models. For example, you could imagine additional columns for use as factors such as: number of previous consecutive months with one or more purchases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       first_event_timestamp  total_purchase_value_on_day  purchase_count_on_day  purchase_value_l30d  purchase_count_l30d  purchase_value_l90d  purchase_count_l90d  purchase_value_l180d  purchase_count_l180d\n",
      "customer_id date                                                                                                                                                                                                                \n",
      "000794e900  2015-06-02            2015-06-02                       145.14                    1.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-03            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-04            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-05            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-06            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-07            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-08            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-09            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-10            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "            2015-06-11            2015-06-02                        39.25                    1.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN\n",
      "...                                      ...                          ...                    ...                  ...                  ...                  ...                  ...                   ...                   ...\n",
      "ffffe95b20  2018-05-24            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-25            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-26            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-27            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-28            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-29            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-30            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-05-31            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-06-01            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "            2018-06-02            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0\n",
      "\n",
      "[6551751 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_time_series_w_rolling_metrics = data_processing.generate_metrics_for_use_as_factors(df_time_series)\n",
    "print df_time_series_w_rolling_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate \"Target\" Column\n",
    "This creates a column representing what we are trying to predict. If a customer made one or more purchases in the 6 months following that day-row, they get a 1 on that day, otherwise, they get a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       first_event_timestamp  total_purchase_value_on_day  purchase_count_on_day  purchase_value_l30d  purchase_count_l30d  purchase_value_l90d  purchase_count_l90d  purchase_value_l180d  purchase_count_l180d  had_purchase_in_future_x_days\n",
      "customer_id date                                                                                                                                                                                                                                               \n",
      "000794e900  2015-06-02            2015-06-02                       145.14                    1.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-03            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-04            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-05            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-06            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-07            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-08            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-09            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-10            2015-06-02                         0.00                    0.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "            2015-06-11            2015-06-02                        39.25                    1.0                  NaN                  NaN                  NaN                  NaN                   NaN                   NaN                              1\n",
      "...                                      ...                          ...                    ...                  ...                  ...                  ...                  ...                   ...                   ...                            ...\n",
      "ffffe95b20  2018-05-24            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-25            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-26            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-27            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-28            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-29            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-30            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-05-31            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-06-01            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "            2018-06-02            2017-08-09                         0.00                    0.0                  0.0                  0.0                  0.0                  0.0                   0.0                   0.0                              0\n",
      "\n",
      "[6551751 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "rolling_window_into_future_in_days =  6*30 # Number of days into the future to look. Set here to 6 months, assuming 30 days per month\n",
    "df_time_series_w_target_col = data_processing.determine_if_event_occured_in_furture_x_days(df_time_series_w_rolling_metrics, 'purchase', rolling_window_into_future_in_days)\n",
    "print df_time_series_w_target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Filter Out Ineligibile Dates & Drop Unneeded Columns\n",
    "From Step 3, we know that we look some number of days into the past to compute rolling metrics (in this case, the largest is 180 days) and in Step 4 we say that we look some number of days into the future for our target column (also 180 days in this scenario). Therefore, some days will be \"too young\" to generate meaningful rolling metrics and some dates will be \"too old\" to have a full 180 window in the future that still exists within the timeframe covered in the dataset.\n",
    "\n",
    "To account for these sets of dates that are \"too young\" or \"too old,\" we generate columns and then use them to filter out ineligible rows to create our final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataframe to /Users/noaflaherty/Documents/GitHub/model-evaluation/notebook/stored_dataframes/TransactionsCompany1.pkl.\n"
     ]
    }
   ],
   "source": [
    "max_rolling_window_span_in_days = 6*30 # This should be set to the same number of days as our longest rolling metric (in this case, 180 days)\n",
    "\n",
    "df_time_series_w_age_viability = data_processing.determine_age_viability_for_model(df_time_series_w_target_col, max_rolling_window_span_in_days, rolling_window_into_future_in_days)\n",
    "df_final = data_processing.get_eligible_training_set(df_time_series_w_age_viability)\n",
    "\n",
    "# Save to a pickle file for easy retrieval later.\n",
    "if pickle_file_of_df:\n",
    "    df_final.to_pickle(pickle_file_of_df)\n",
    "    print \"Saved dataframe to %s.\" % pickle_file_of_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Final Dataframe Output\n",
    "This is the final output of Phase 1. It is a clean time-series'ed dataframe that can be used for training or evaluating models.\n",
    "\n",
    "You can either run all cells above, or just this one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataframe from pickle file at /Users/noaflaherty/Documents/GitHub/model-evaluation/notebook/stored_dataframes/TransactionsCompany1.pkl\n",
      "\n",
      "                        purchase_value_l30d  purchase_count_l30d  purchase_value_l90d  purchase_count_l90d  purchase_value_l180d  purchase_count_l180d  had_purchase_in_future_x_days\n",
      "customer_id date                                                                                                                                                                     \n",
      "000794e900  2015-11-28         1.421085e-14                  0.0         1.421085e-14                  0.0          2.865000e+02                   3.0                              0\n",
      "            2015-11-29         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-11-30         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-01         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-02         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-03         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-04         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-05         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-06         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "            2015-12-07         1.421085e-14                  0.0         1.421085e-14                  0.0          1.413600e+02                   2.0                              0\n",
      "...                                     ...                  ...                  ...                  ...                   ...                   ...                            ...\n",
      "fffcaa3024  2017-11-25         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-11-26         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-11-27         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-11-28         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-11-29         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-11-30         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-12-01         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-12-02         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-12-03         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "            2017-12-04         0.000000e+00                  0.0        -7.105427e-15                  0.0         -7.105427e-15                   0.0                              0\n",
      "\n",
      "[2673663 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df_final = data_processing.load_dataframe(pickle_file_of_df, csv=input_csv, overwrite_pickle=False)\n",
    "print df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Phase 2: Dummy Model Creation\n",
    "\n",
    "For now, we will simply create two very similar Logistic Regression models, who differ only in their seed and training/test split values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "model_paths = [model_1_file, model_2_file]\n",
    "models = model_evaluation.load_models(df_final, model_paths, generate_new=True) # generate_new can be set to False to simply load models from the inputs at the top.\n",
    "print models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Model Evaluation & Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataframe from pickle file at /Users/noaflaherty/Documents/GitHub/model-evaluation/notebook/stored_dataframes/TransactionsCompany1.pkl\n",
      "\n",
      "###########################################################\n",
      "#########     Evaluation Metrics: Model 1      ############\n",
      "###########################################################\n",
      "\n",
      "Accuracy Score:\n",
      "0.8297623896504533\n",
      "\n",
      "Log Loss Score:\n",
      "5.879806347523355\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2180224   25378]\n",
      " [ 429780   38281]]\n",
      "\n",
      "Confusion Matrix (As Percents):\n",
      "[[0.81544458 0.00949185]\n",
      " [0.16074576 0.01431781]]\n",
      "\n",
      "Area Under the Receiver Operating Characteristic Curve:\n",
      "0.5351400963357816\n",
      "\n",
      "F1 Score:\n",
      "0.14398931768600015\n",
      "\n",
      "Mean Absolute Error:\n",
      "0.17023761034954668\n",
      "\n",
      "Mean Squared Error:\n",
      "0.17023761034954668\n",
      "\n",
      "\n",
      "\n",
      "###########################################################\n",
      "#########     Evaluation Metrics: Model 2      ############\n",
      "###########################################################\n",
      "\n",
      "Accuracy Score:\n",
      "0.8297799685300653\n",
      "\n",
      "Log Loss Score:\n",
      "5.879199210082514\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2180172   25430]\n",
      " [ 429681   38380]]\n",
      "\n",
      "Confusion Matrix (As Percents):\n",
      "[[0.81542513 0.0095113 ]\n",
      " [0.16070874 0.01435484]]\n",
      "\n",
      "Area Under the Receiver Operating Characteristic Curve:\n",
      "0.5352340636173232\n",
      "\n",
      "F1 Score:\n",
      "0.14432070934493516\n",
      "\n",
      "Mean Absolute Error:\n",
      "0.1702200314699347\n",
      "\n",
      "Mean Squared Error:\n",
      "0.1702200314699347\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_evaluation.compare_models(input_csv, model_paths, df_pickle_file=PICKLE_FILE_OF_DF, generate_new_models=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
